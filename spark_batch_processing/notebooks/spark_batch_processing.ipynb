{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a67d7b-b153-4167-b8dc-9eea274b9171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m996.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bd3fd5-c9de-472e-997d-7958bc3ed5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# initialize spark session\n",
    "spark=SparkSession.builder \\\n",
    "    .appName(\"Spark_Batch_Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0eeeab-758f-4706-8790-46186587c6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2f376a3ccf71:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark_Batch_Processing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b1d80537850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec611f1-3138-4233-9820-d14a41dfa7b6",
   "metadata": {},
   "source": [
    "### load data into an RDD and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63c81a8-50b8-4a1f-9445-6fcd1dff5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ADMISSIONS.csv file into an RDD\n",
    "rdd=spark.sparkContext.textFile(\"hdfs://namenode:9000/data/ADMISSIONS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c390404-39fa-4314-bf9f-0d11581a2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the header (first row)\n",
    "header=rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3f26bb-0283-4e52-b8c5-8037d3b66394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the RDD (as a list of columns):\n",
      "['1', '10001', '20001', '2021-01-01 08:00:00', '2021-01-10 12:00:00', '', 'EMERGENCY', 'EMERGENCY ROOM', 'HOME', 'Medicare', 'ENGL', 'CATHOLIC', 'MARRIED', 'WHITE', '2021-01-01 07:30:00', '2021-01-01 08:30:00', 'PNEUMONIA', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "# Filter out the header and split rows ino columns\n",
    "rows_rdd=rdd.filter(lambda line:line !=header).map(lambda line:line.split(\",\"))\n",
    "# verify the structure of the rdd\n",
    "print(\"First row of the RDD (as a list of columns):\")\n",
    "first_row=rows_rdd.first()\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27cf197c-2793-42f4-8f22-c723d5566a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column indices and values:\n",
      "Column 0: 1\n",
      "Column 1: 10001\n",
      "Column 2: 20001\n",
      "Column 3: 2021-01-01 08:00:00\n",
      "Column 4: 2021-01-10 12:00:00\n",
      "Column 5: \n",
      "Column 6: EMERGENCY\n",
      "Column 7: EMERGENCY ROOM\n",
      "Column 8: HOME\n",
      "Column 9: Medicare\n",
      "Column 10: ENGL\n",
      "Column 11: CATHOLIC\n",
      "Column 12: MARRIED\n",
      "Column 13: WHITE\n",
      "Column 14: 2021-01-01 07:30:00\n",
      "Column 15: 2021-01-01 08:30:00\n",
      "Column 16: PNEUMONIA\n",
      "Column 17: 0\n",
      "Column 18: 1\n"
     ]
    }
   ],
   "source": [
    "# print the index and value of each column\n",
    "print(\"Column indices and values:\")\n",
    "for idx,value in enumerate(first_row):\n",
    "    print(f\"Column {idx}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c32a325-39e5-4c12-8ccf-81ec586a8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on admission type (e.g., \"EMERGENCY\")\n",
    "# Assuming ADMISSION_TYPE is at index 6\n",
    "filtered_rdd=rows_rdd.filter(lambda row:row[6]==\"EMERGENCY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453e23f4-79da-42ee-8a26-99839ae80197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows:\n",
      "['1', '10001', '20001', '2021-01-01 08:00:00', '2021-01-10 12:00:00', '', 'EMERGENCY', 'EMERGENCY ROOM', 'HOME', 'Medicare', 'ENGL', 'CATHOLIC', 'MARRIED', 'WHITE', '2021-01-01 07:30:00', '2021-01-01 08:30:00', 'PNEUMONIA', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "# Show the filtered RDD\n",
    "print(\"Filtered rows:\")\n",
    "for row in filtered_rdd.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52747a2-3217-445a-9b05-ccaad1c9a796",
   "metadata": {},
   "source": [
    "### load data and filter using Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8117baf2-816b-4350-9fa3-64c237ee5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\",True).load(\"hdfs://namenode:9000/data/ADMISSIONS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0127eebf-6098-4365-b936-7cd5b8b71cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+-------------------+--------------+--------------------+--------------------+---------+--------+-----------------+--------------+--------------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|          DEATHTIME|ADMISSION_TYPE|  ADMISSION_LOCATION|  DISCHARGE_LOCATION|INSURANCE|LANGUAGE|         RELIGION|MARITAL_STATUS|           ETHNICITY|          EDREGTIME|          EDOUTTIME|   DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+-------------------+--------------+--------------------+--------------------+---------+--------+-----------------+--------------+--------------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|     1|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|               NULL|     EMERGENCY|      EMERGENCY ROOM|                HOME| Medicare|    ENGL|         CATHOLIC|       MARRIED|               WHITE|2021-01-01 07:30:00|2021-01-01 08:30:00|   PNEUMONIA|                   0|                   1|\n",
      "|     2|     10002|  20002|2019-11-20 14:30:00|2019-11-25 10:00:00|2020-03-01 00:00:00|        URGENT|       PHYS REFERRAL|             EXPIRED|  Private|    ENGL|PROTESTANT QUAKER|        SINGLE|BLACK/AFRICAN AME...|2019-11-20 14:00:00|2019-11-20 15:00:00|      STROKE|                   1|                   1|\n",
      "|     3|     10003|  20003|2022-06-15 10:00:00|2022-06-20 09:00:00|               NULL|      ELECTIVE|TRANSFER FROM HOS...|REHAB/DISTINCT PA...| Self Pay|    ENGL|     UNOBTAINABLE|      DIVORCED|     HISPANIC/LATINO|2022-06-15 09:30:00|2022-06-15 10:30:00|APPENDICITIS|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+-------------------+--------------+--------------------+--------------------+---------+--------+-----------------+--------------+--------------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de92172a-5454-4b00-a739-0bde728e0045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ROW_ID: int, SUBJECT_ID: int, HADM_ID: int, ADMITTIME: timestamp, DISCHTIME: timestamp, DEATHTIME: timestamp, ADMISSION_TYPE: string, ADMISSION_LOCATION: string, DISCHARGE_LOCATION: string, INSURANCE: string, LANGUAGE: string, RELIGION: string, MARITAL_STATUS: string, ETHNICITY: string, EDREGTIME: timestamp, EDOUTTIME: timestamp, DIAGNOSIS: string, HOSPITAL_EXPIRE_FLAG: int, HAS_CHARTEVENTS_DATA: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b832e8f0-ced7-4780-b8b5-4e3a050d987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|admission_type|\n",
      "+--------------+\n",
      "|      ELECTIVE|\n",
      "|     EMERGENCY|\n",
      "|        URGENT|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('admission_type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bcebb61-c813-4300-97cb-fcf9b8eba54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+---------+--------------+--------------------+--------------------+---------+--------+------------+--------------+---------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|DEATHTIME|ADMISSION_TYPE|  ADMISSION_LOCATION|  DISCHARGE_LOCATION|INSURANCE|LANGUAGE|    RELIGION|MARITAL_STATUS|      ETHNICITY|          EDREGTIME|          EDOUTTIME|   DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+--------------------+--------------------+---------+--------+------------+--------------+---------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|     3|     10003|  20003|2022-06-15 10:00:00|2022-06-20 09:00:00|     NULL|      ELECTIVE|TRANSFER FROM HOS...|REHAB/DISTINCT PA...| Self Pay|    ENGL|UNOBTAINABLE|      DIVORCED|HISPANIC/LATINO|2022-06-15 09:30:00|2022-06-15 10:30:00|APPENDICITIS|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+--------------------+--------------------+---------+--------+------------+--------------+---------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elective_patients=df.filter(df['admission_type']=='ELECTIVE')\n",
    "elective_patients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fa447da-fee8-420b-a13e-9ce2ad5835da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|DEATHTIME|ADMISSION_TYPE|ADMISSION_LOCATION|DISCHARGE_LOCATION|INSURANCE|LANGUAGE|RELIGION|MARITAL_STATUS|ETHNICITY|          EDREGTIME|          EDOUTTIME|DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|     1|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|     NULL|     EMERGENCY|    EMERGENCY ROOM|              HOME| Medicare|    ENGL|CATHOLIC|       MARRIED|    WHITE|2021-01-01 07:30:00|2021-01-01 08:30:00|PNEUMONIA|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emergency_patients = df.filter(df[\"admission_type\"] == 'EMERGENCY')\n",
    "emergency_patients.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dbf4a-e4d9-4996-8011-c4e4cf57b03b",
   "metadata": {},
   "source": [
    "### load into dataFrame and filter using spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2124dd4-3a4f-42f5-8b4d-b4956843521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register df as temporary view to run SparkSQL commands\n",
    "df.createOrReplaceTempView('admissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7193a82b-631d-4923-a601-5d97a88e7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|DEATHTIME|ADMISSION_TYPE|ADMISSION_LOCATION|DISCHARGE_LOCATION|INSURANCE|LANGUAGE|RELIGION|MARITAL_STATUS|ETHNICITY|          EDREGTIME|          EDOUTTIME|DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|     1|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|     NULL|     EMERGENCY|    EMERGENCY ROOM|              HOME| Medicare|    ENGL|CATHOLIC|       MARRIED|    WHITE|2021-01-01 07:30:00|2021-01-01 08:30:00|PNEUMONIA|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For example, to filter patients with an admission type of \"EMERGENCY\"\n",
    "emergency_patients=spark.sql(\"\"\"\n",
    "        select *\n",
    "        from admissions\n",
    "        where ADMISSION_TYPE = 'EMERGENCY'\n",
    "\"\"\")\n",
    "emergency_patients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb8c41b6-1aed-41fa-9312-6a96b33cd9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data as csv file in HDFS\n",
    "emergency_patients.write.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"hdfs://namenode:9000/data/emergency_patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd2d79-72c5-4371-9c64-0d4e77408cd5",
   "metadata": {},
   "source": [
    "### Aggregating data and calculating statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa75fec0-b9dd-4f94-8188-297028eca3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Admissions: 3\n"
     ]
    }
   ],
   "source": [
    "total_admissions=df.count()\n",
    "print(\"Total Admissions:\", total_admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33153979-008e-45fd-9422-6eeaa265cc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Patients: 3\n"
     ]
    }
   ],
   "source": [
    "unique_patients=df.select(\"SUBJECT_ID\").distinct().count()\n",
    "print(\"Unique Patients:\", unique_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "907f3ad3-714c-4945-90c0-638204b9a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|ADMISSION_TYPE|ADMISSION_COUNT|\n",
      "+--------------+---------------+\n",
      "|      ELECTIVE|              1|\n",
      "|     EMERGENCY|              1|\n",
      "|        URGENT|              1|\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.groupby(\"ADMISSION_TYPE\").agg(count(\"*\").alias(\"ADMISSION_COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9257907d-6c8a-4d08-b201-5aa65ff19c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+-------------------+--------------+\n",
      "|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|LENGTH_OF_STAY|\n",
      "+----------+-------+-------------------+-------------------+--------------+\n",
      "|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|             9|\n",
      "|     10002|  20002|2019-11-20 14:30:00|2019-11-25 10:00:00|             5|\n",
      "|     10003|  20003|2022-06-15 10:00:00|2022-06-20 09:00:00|             5|\n",
      "+----------+-------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,unix_timestamp,datediff\n",
    "\n",
    "# Convert ADMITTIME and DISCHTIME to timestamps\n",
    "df=df.withColumn(\"ADMITTIME\",col(\"ADMITTIME\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"DISCHTIME\",col(\"DISCHTIME\").cast(\"timestamp\"))\n",
    "# Calculate length of stay in days\n",
    "df = df.withColumn(\"LENGTH_OF_STAY\", datediff(col(\"DISCHTIME\"), col(\"ADMITTIME\")))\n",
    "# Show the DataFrame with the new LENGTH_OF_STAY column\n",
    "df.select(\"SUBJECT_ID\", \"HADM_ID\", \"ADMITTIME\", \"DISCHTIME\", \"LENGTH_OF_STAY\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30b66cfe-c6b5-4b48-8a19-353353dd0534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Length of Stay (in days): 5.0\n"
     ]
    }
   ],
   "source": [
    "median_los=df.approxQuantile(\"LENGTH_OF_STAY\",[.5],.01)\n",
    "print(\"Median Length of Stay (in days):\", median_los[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e01bb7-7178-47ba-8f3e-4edd4e89c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Length of Stay (in days): 5\n",
      "Maximum Length of Stay (in days): 9\n",
      "Mean Length of Stay (in days): 6.333333333333333\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "\n",
    "min_los = df.select(min(\"LENGTH_OF_STAY\")).collect()[0][0]\n",
    "max_los = df.select(max(\"LENGTH_OF_STAY\")).collect()[0][0]\n",
    "mean_los = df.select(mean(\"LENGTH_OF_STAY\")).collect()[0][0]\n",
    "\n",
    "print(\"Minimum Length of Stay (in days):\", min_los)\n",
    "print(\"Maximum Length of Stay (in days):\", max_los)\n",
    "print(\"Mean Length of Stay (in days):\", mean_los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "131c9d6e-e470-4ea0-85d4-92ab792d0747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+-------+\n",
      "|ADMISSION_TYPE|AVG_LOS|MIN_LOS|MAX_LOS|\n",
      "+--------------+-------+-------+-------+\n",
      "|      ELECTIVE|    5.0|      5|      5|\n",
      "|     EMERGENCY|    9.0|      9|      9|\n",
      "|        URGENT|    5.0|      5|      5|\n",
      "+--------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df.groupby(\"ADMISSION_TYPE\").agg(\n",
    "    avg(\"LENGTH_OF_STAY\").alias(\"AVG_LOS\"),\n",
    "    min(\"LENGTH_OF_STAY\").alias(\"MIN_LOS\"),\n",
    "    max(\"LENGTH_OF_STAY\").alias(\"MAX_LOS\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463de40-5bd7-4f38-b61d-3f45cd0f29f8",
   "metadata": {},
   "source": [
    "####  Mortality Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db8909dd-f100-4132-a886-8c18757e120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Hospital Mortality Rate: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "mortality_rate=df.filter(col(\"HOSPITAL_EXPIRE_FLAG\")==1).count()/total_admissions\n",
    "print(\"In-Hospital Mortality Rate:\", mortality_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407cd69c-7086-4d49-a2d3-3bdbc93570c3",
   "metadata": {},
   "source": [
    "####  Admission Location and Discharge Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dc52f51-c603-4d4e-a8a5-35c53e662c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|  ADMISSION_LOCATION|COUNT|\n",
      "+--------------------+-----+\n",
      "|      EMERGENCY ROOM|    1|\n",
      "|       PHYS REFERRAL|    1|\n",
      "|TRANSFER FROM HOS...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Most Common Admission Locations\n",
    "df.groupBy(\"ADMISSION_LOCATION\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f678b-9b71-41f0-a267-450ad99184c3",
   "metadata": {},
   "source": [
    "####  Insurance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c226e98c-6745-4066-b3b4-37583bccc037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|INSURANCE|COUNT|\n",
      "+---------+-----+\n",
      "| Self Pay|    1|\n",
      "|  Private|    1|\n",
      "| Medicare|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution of Insurance Types:\n",
    "df.groupby(\"INSURANCE\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "775cfa3a-3a2d-4871-a185-e832df9ebcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "|INSURANCE|AVG_LOS|MIN_LOS|MAX_LOS|\n",
      "+---------+-------+-------+-------+\n",
      "| Self Pay|    5.0|      5|      5|\n",
      "|  Private|    5.0|      5|      5|\n",
      "| Medicare|    9.0|      9|      9|\n",
      "+---------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Length of Stay by Insurance Type:\n",
    "# Length of Stay by Insurance Type:\n",
    "df.groupBy(\"INSURANCE\").agg(\n",
    "    avg(\"LENGTH_OF_STAY\").alias(\"AVG_LOS\"),\n",
    "    min(\"LENGTH_OF_STAY\").alias(\"MIN_LOS\"),\n",
    "    max(\"LENGTH_OF_STAY\").alias(\"MAX_LOS\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19205af1-eb9d-46ea-9ecd-6c26ead868c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|LANGUAGE|COUNT|\n",
      "+--------+-----+\n",
      "|    ENGL|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most Common Languages:\n",
    "df.groupBy(\"LANGUAGE\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee6f4796-b0a5-452a-a977-46dab83033a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|DIAGNOSIS   |COUNT|\n",
      "+------------+-----+\n",
      "|APPENDICITIS|1    |\n",
      "|PNEUMONIA   |1    |\n",
      "|STROKE      |1    |\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Diagnosis Statistics\n",
    "# Most Common Diagnoses:\n",
    "df.groupBy(\"DIAGNOSIS\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d83decca-4f4c-47e7-8e34-bd8340191bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+-------+\n",
      "|DIAGNOSIS   |AVG_LOS|MIN_LOS|MAX_LOS|\n",
      "+------------+-------+-------+-------+\n",
      "|PNEUMONIA   |9.0    |9      |9      |\n",
      "|APPENDICITIS|5.0    |5      |5      |\n",
      "|STROKE      |5.0    |5      |5      |\n",
      "+------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Length of Stay by Diagnosis:\n",
    "df.groupBy(\"DIAGNOSIS\").agg(\n",
    "    avg(\"LENGTH_OF_STAY\").alias(\"AVG_LOS\"),\n",
    "    min(\"LENGTH_OF_STAY\").alias(\"MIN_LOS\"),\n",
    "    max(\"LENGTH_OF_STAY\").alias(\"MAX_LOS\")\n",
    ").orderBy(desc(\"AVG_LOS\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed03a8-0e4f-4a93-b6bc-517c43e700cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
