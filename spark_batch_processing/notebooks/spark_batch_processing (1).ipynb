{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd3b747-0c2a-4eab-9a54-22b20e20d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_Batch_Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506a296e-6f10-4ee6-b657-e3da44aa92ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4d7b727f97ac:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark_Batch_Processing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x73b84792f350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fada2b-0416-4521-b7dc-08f0d5cc2675",
   "metadata": {},
   "source": [
    "# 1. load data into an RDD and filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d63bed-67ac-4a01-b0bf-cad823c77af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ADMISSIONS.csv file into an RDD\n",
    "rdd = spark.sparkContext.textFile(\"hdfs://namenode:9000/data/ADMISSIONS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e95506e-3844-42e9-9ea8-b2b410b0aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the header (first row)\n",
    "header = rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89482f58-4766-48df-a69d-f82b926ca30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the RDD (as a list of columns):\n",
      "['1', '10001', '20001', '2021-01-01 08:00:00', '2021-01-10 12:00:00', '', 'EMERGENCY', 'EMERGENCY ROOM', 'HOME', 'Medicare', 'ENGL', 'CATHOLIC', 'MARRIED', 'WHITE', '2021-01-01 07:30:00', '2021-01-01 08:30:00', 'PNEUMONIA', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "# Filter out the header and split rows into columns\n",
    "rows_rdd = rdd.filter(lambda line: line != header).map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Verify the structure of the RDD\n",
    "print(\"First row of the RDD (as a list of columns):\")\n",
    "first_row = rows_rdd.first()\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161373dd-3384-42b4-99fe-17fe5a7826fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column indices and values:\n",
      "Column 0: 1\n",
      "Column 1: 10001\n",
      "Column 2: 20001\n",
      "Column 3: 2021-01-01 08:00:00\n",
      "Column 4: 2021-01-10 12:00:00\n",
      "Column 5: \n",
      "Column 6: EMERGENCY\n",
      "Column 7: EMERGENCY ROOM\n",
      "Column 8: HOME\n",
      "Column 9: Medicare\n",
      "Column 10: ENGL\n",
      "Column 11: CATHOLIC\n",
      "Column 12: MARRIED\n",
      "Column 13: WHITE\n",
      "Column 14: 2021-01-01 07:30:00\n",
      "Column 15: 2021-01-01 08:30:00\n",
      "Column 16: PNEUMONIA\n",
      "Column 17: 0\n",
      "Column 18: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the index and value of each column\n",
    "print(\"Column indices and values:\")\n",
    "for idx, value in enumerate(first_row):\n",
    "    print(f\"Column {idx}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d094d171-30ba-4533-961e-a7b766cdd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on admission type (e.g., \"EMERGENCY\")\n",
    "# Assuming ADMISSION_TYPE is at index 6\n",
    "filtered_rdd = rows_rdd.filter(lambda row: row[6] =='EMERGENCY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "821012bd-fee0-4503-903f-f26cb4a67ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows:\n",
      "['1', '10001', '20001', '2021-01-01 08:00:00', '2021-01-10 12:00:00', '', 'EMERGENCY', 'EMERGENCY ROOM', 'HOME', 'Medicare', 'ENGL', 'CATHOLIC', 'MARRIED', 'WHITE', '2021-01-01 07:30:00', '2021-01-01 08:30:00', 'PNEUMONIA', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "# Show the filtered RDD\n",
    "print(\"Filtered rows:\")\n",
    "for row in filtered_rdd.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac850e1-0992-4a8f-95e0-a7b216abcbb1",
   "metadata": {},
   "source": [
    "# 2. load data and filter using Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a1053b4-e75a-4922-977f-a822f06f52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"hdfs://namenode:9000/data/ADMISSIONS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44bf697f-ab55-4023-86d4-6ea12c0bc42f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+-------------------+--------------+--------------------+--------------------+---------+--------+-----------------+--------------+--------------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|          DEATHTIME|ADMISSION_TYPE|  ADMISSION_LOCATION|  DISCHARGE_LOCATION|INSURANCE|LANGUAGE|         RELIGION|MARITAL_STATUS|           ETHNICITY|          EDREGTIME|          EDOUTTIME|   DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+-------------------+--------------+--------------------+--------------------+---------+--------+-----------------+--------------+--------------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|     1|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|               NULL|     EMERGENCY|      EMERGENCY ROOM|                HOME| Medicare|    ENGL|         CATHOLIC|       MARRIED|               WHITE|2021-01-01 07:30:00|2021-01-01 08:30:00|   PNEUMONIA|                   0|                   1|\n",
      "|     2|     10002|  20002|2019-11-20 14:30:00|2019-11-25 10:00:00|2020-03-01 00:00:00|        URGENT|       PHYS REFERRAL|             EXPIRED|  Private|    ENGL|PROTESTANT QUAKER|        SINGLE|BLACK/AFRICAN AME...|2019-11-20 14:00:00|2019-11-20 15:00:00|      STROKE|                   1|                   1|\n",
      "|     3|     10003|  20003|2022-06-15 10:00:00|2022-06-20 09:00:00|               NULL|      ELECTIVE|TRANSFER FROM HOS...|REHAB/DISTINCT PA...| Self Pay|    ENGL|     UNOBTAINABLE|      DIVORCED|     HISPANIC/LATINO|2022-06-15 09:30:00|2022-06-15 10:30:00|APPENDICITIS|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+-------------------+--------------+--------------------+--------------------+---------+--------+-----------------+--------------+--------------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fbcdb9-01ff-4212-906a-98caf9770a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ROW_ID: int, SUBJECT_ID: int, HADM_ID: int, ADMITTIME: timestamp, DISCHTIME: timestamp, DEATHTIME: timestamp, ADMISSION_TYPE: string, ADMISSION_LOCATION: string, DISCHARGE_LOCATION: string, INSURANCE: string, LANGUAGE: string, RELIGION: string, MARITAL_STATUS: string, ETHNICITY: string, EDREGTIME: timestamp, EDOUTTIME: timestamp, DIAGNOSIS: string, HOSPITAL_EXPIRE_FLAG: int, HAS_CHARTEVENTS_DATA: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f49573-cb94-4857-86fc-7b5eda920758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|admission_type|\n",
      "+--------------+\n",
      "|      ELECTIVE|\n",
      "|     EMERGENCY|\n",
      "|        URGENT|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('admission_type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f60f4a8f-746b-47c7-a5bc-bb57e58a89b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+---------+--------------+--------------------+--------------------+---------+--------+------------+--------------+---------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|DEATHTIME|ADMISSION_TYPE|  ADMISSION_LOCATION|  DISCHARGE_LOCATION|INSURANCE|LANGUAGE|    RELIGION|MARITAL_STATUS|      ETHNICITY|          EDREGTIME|          EDOUTTIME|   DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+--------------------+--------------------+---------+--------+------------+--------------+---------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "|     3|     10003|  20003|2022-06-15 10:00:00|2022-06-20 09:00:00|     NULL|      ELECTIVE|TRANSFER FROM HOS...|REHAB/DISTINCT PA...| Self Pay|    ENGL|UNOBTAINABLE|      DIVORCED|HISPANIC/LATINO|2022-06-15 09:30:00|2022-06-15 10:30:00|APPENDICITIS|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+--------------------+--------------------+---------+--------+------------+--------------+---------------+-------------------+-------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elective_patients = df.filter(df[\"admission_type\"] == 'ELECTIVE')\n",
    "elective_patients.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36ce4666-1869-421a-b37e-20f9ec173390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|DEATHTIME|ADMISSION_TYPE|ADMISSION_LOCATION|DISCHARGE_LOCATION|INSURANCE|LANGUAGE|RELIGION|MARITAL_STATUS|ETHNICITY|          EDREGTIME|          EDOUTTIME|DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|     1|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|     NULL|     EMERGENCY|    EMERGENCY ROOM|              HOME| Medicare|    ENGL|CATHOLIC|       MARRIED|    WHITE|2021-01-01 07:30:00|2021-01-01 08:30:00|PNEUMONIA|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emergency_patients = df.filter(df[\"admission_type\"] == 'EMERGENCY')\n",
    "emergency_patients.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d292f9-61f3-432f-b4fa-a161388bddf7",
   "metadata": {},
   "source": [
    "# 3. load into dataFrame and filter using spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f093b8e-54b1-42d6-8944-9f738630ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register df as temporary view to run SparkSQL commands\n",
    "df.createOrReplaceTempView(\"admissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bedec86-686c-4251-9e0e-e9f3d52ab876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|ROW_ID|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|DEATHTIME|ADMISSION_TYPE|ADMISSION_LOCATION|DISCHARGE_LOCATION|INSURANCE|LANGUAGE|RELIGION|MARITAL_STATUS|ETHNICITY|          EDREGTIME|          EDOUTTIME|DIAGNOSIS|HOSPITAL_EXPIRE_FLAG|HAS_CHARTEVENTS_DATA|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|     1|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|     NULL|     EMERGENCY|    EMERGENCY ROOM|              HOME| Medicare|    ENGL|CATHOLIC|       MARRIED|    WHITE|2021-01-01 07:30:00|2021-01-01 08:30:00|PNEUMONIA|                   0|                   1|\n",
      "+------+----------+-------+-------------------+-------------------+---------+--------------+------------------+------------------+---------+--------+--------+--------------+---------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For example, to filter patients with an admission type of \"EMERGENCY\"\n",
    "emergency_patients = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM admissions\n",
    "    WHERE ADMISSION_TYPE = 'EMERGENCY'\n",
    "\"\"\")\n",
    "emergency_patients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d54f0cc-bc1e-4a7c-ae06-51ff9a9e4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered data as csv file in HDFS\n",
    "emergency_patients.write.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"hdfs://namenode:9000/data/emergency_patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360122a-59c2-45eb-9f17-818dcac4c8ce",
   "metadata": {},
   "source": [
    "# Part 2: Aggregating data and calculating statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517e4d6-659e-4064-b426-5a77ff6e9011",
   "metadata": {},
   "source": [
    "## 1. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87b2fccb-218d-4972-89ed-5b7590a27f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Admissions: 3\n"
     ]
    }
   ],
   "source": [
    "total_admissions = df.count()\n",
    "print(\"Total Admissions:\", total_admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dba6f7b4-02a1-4123-b568-199536adea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Patients: 3\n"
     ]
    }
   ],
   "source": [
    "unique_patients = df.select(\"SUBJECT_ID\").distinct().count()\n",
    "print(\"Unique Patients:\", unique_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97ca3534-ceae-4651-b50c-322c04809265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|ADMISSION_TYPE|ADMISSION_COUNT|\n",
      "+--------------+---------------+\n",
      "|      ELECTIVE|              1|\n",
      "|     EMERGENCY|              1|\n",
      "|        URGENT|              1|\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"ADMISSION_TYPE\").agg(count(\"*\").alias(\"ADMISSION_COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1e124a-b282-4d70-8fa0-a827e838cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88cee0c9-0e9e-4c92-9db8-5021478dd0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+-------------------+--------------+\n",
      "|SUBJECT_ID|HADM_ID|          ADMITTIME|          DISCHTIME|LENGTH_OF_STAY|\n",
      "+----------+-------+-------------------+-------------------+--------------+\n",
      "|     10001|  20001|2021-01-01 08:00:00|2021-01-10 12:00:00|             9|\n",
      "|     10002|  20002|2019-11-20 14:30:00|2019-11-25 10:00:00|             5|\n",
      "|     10003|  20003|2022-06-15 10:00:00|2022-06-20 09:00:00|             5|\n",
      "+----------+-------+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, datediff\n",
    "\n",
    "# Convert ADMITTIME and DISCHTIME to timestamps\n",
    "df = df.withColumn(\"ADMITTIME\", col(\"ADMITTIME\").cast(\"timestamp\")) \\\n",
    "       .withColumn(\"DISCHTIME\", col(\"DISCHTIME\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate length of stay in days\n",
    "df = df.withColumn(\"LENGTH_OF_STAY\", datediff(col(\"DISCHTIME\"), col(\"ADMITTIME\")))\n",
    "\n",
    "# Show the DataFrame with the new LENGTH_OF_STAY column\n",
    "df.select(\"SUBJECT_ID\", \"HADM_ID\", \"ADMITTIME\", \"DISCHTIME\", \"LENGTH_OF_STAY\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c0c01d5-84d2-4fd8-b75a-d94f0a3696c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Length of Stay (in days): 5.0\n"
     ]
    }
   ],
   "source": [
    "median_los = df.approxQuantile(\"LENGTH_OF_STAY\", [0.5], 0.01)\n",
    "print(\"Median Length of Stay (in days):\", median_los[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edffbe6c-394c-46ee-8dd0-6944db3657af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Length of Stay (in days): 5\n",
      "Maximum Length of Stay (in days): 9\n",
      "Mean Length of Stay (in days): 6.333333333333333\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "\n",
    "min_los = df.select(min(\"LENGTH_OF_STAY\")).collect()[0][0]\n",
    "max_los = df.select(max(\"LENGTH_OF_STAY\")).collect()[0][0]\n",
    "mean_los = df.select(mean(\"LENGTH_OF_STAY\")).collect()[0][0]\n",
    "\n",
    "print(\"Minimum Length of Stay (in days):\", min_los)\n",
    "print(\"Maximum Length of Stay (in days):\", max_los)\n",
    "print(\"Mean Length of Stay (in days):\", mean_los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "056b9590-d746-473d-8a01-a3594949f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+-------+\n",
      "|ADMISSION_TYPE|AVG_LOS|MIN_LOS|MAX_LOS|\n",
      "+--------------+-------+-------+-------+\n",
      "|      ELECTIVE|    5.0|      5|      5|\n",
      "|     EMERGENCY|    9.0|      9|      9|\n",
      "|        URGENT|    5.0|      5|      5|\n",
      "+--------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"ADMISSION_TYPE\").agg(\n",
    "    avg(\"LENGTH_OF_STAY\").alias(\"AVG_LOS\"),\n",
    "    min(\"LENGTH_OF_STAY\").alias(\"MIN_LOS\"),\n",
    "    max(\"LENGTH_OF_STAY\").alias(\"MAX_LOS\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653305fa-cbeb-4f14-afca-e15074aea17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mortality Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ebd7951-a334-4ce4-9088-d4970e19ffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Hospital Mortality Rate: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "mortality_rate = df.filter(col(\"HOSPITAL_EXPIRE_FLAG\") == 1).count() / total_admissions\n",
    "print(\"In-Hospital Mortality Rate:\", mortality_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35df3b8e-78f6-49db-a996-e498a45483fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Admission Location and Discharge Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "485dba40-5415-4996-a31e-9e5c0f46f5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|  ADMISSION_LOCATION|COUNT|\n",
      "+--------------------+-----+\n",
      "|      EMERGENCY ROOM|    1|\n",
      "|       PHYS REFERRAL|    1|\n",
      "|TRANSFER FROM HOS...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Most Common Admission Locations\n",
    "df.groupBy(\"ADMISSION_LOCATION\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fb03668-5fa1-4195-a5d9-305581d63409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Insurance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e33ec7a0-b9a0-437f-921a-e082e3d189d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|INSURANCE|COUNT|\n",
      "+---------+-----+\n",
      "| Self Pay|    1|\n",
      "|  Private|    1|\n",
      "| Medicare|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution of Insurance Types:\n",
    "df.groupBy(\"INSURANCE\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00216cd8-9ceb-4e3b-9770-5d92e9b4d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "|INSURANCE|AVG_LOS|MIN_LOS|MAX_LOS|\n",
      "+---------+-------+-------+-------+\n",
      "| Self Pay|    5.0|      5|      5|\n",
      "|  Private|    5.0|      5|      5|\n",
      "| Medicare|    9.0|      9|      9|\n",
      "+---------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Length of Stay by Insurance Type:\n",
    "df.groupBy(\"INSURANCE\").agg(\n",
    "    avg(\"LENGTH_OF_STAY\").alias(\"AVG_LOS\"),\n",
    "    min(\"LENGTH_OF_STAY\").alias(\"MIN_LOS\"),\n",
    "    max(\"LENGTH_OF_STAY\").alias(\"MAX_LOS\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d42517f6-a1b4-4ff6-a633-6d4b9ce9f3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           ETHNICITY|COUNT|\n",
      "+--------------------+-----+\n",
      "|               WHITE|    1|\n",
      "|     HISPANIC/LATINO|    1|\n",
      "|BLACK/AFRICAN AME...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Ethnicity and Language Statistics\n",
    "# Distribution of Ethnicities:\n",
    "df.groupBy(\"ETHNICITY\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "835e1bf3-53de-4767-b6b0-6bda968b6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|LANGUAGE|COUNT|\n",
      "+--------+-----+\n",
      "|    ENGL|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most Common Languages:\n",
    "df.groupBy(\"LANGUAGE\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b007e630-dbed-466a-bce9-22ebb93f7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|DIAGNOSIS   |COUNT|\n",
      "+------------+-----+\n",
      "|APPENDICITIS|1    |\n",
      "|PNEUMONIA   |1    |\n",
      "|STROKE      |1    |\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Diagnosis Statistics\n",
    "# Most Common Diagnoses:\n",
    "df.groupBy(\"DIAGNOSIS\").agg(count(\"*\").alias(\"COUNT\")).orderBy(desc(\"COUNT\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6982c51f-e902-432a-a102-08b16eb1ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+-------+\n",
      "|DIAGNOSIS   |AVG_LOS|MIN_LOS|MAX_LOS|\n",
      "+------------+-------+-------+-------+\n",
      "|PNEUMONIA   |9.0    |9      |9      |\n",
      "|APPENDICITIS|5.0    |5      |5      |\n",
      "|STROKE      |5.0    |5      |5      |\n",
      "+------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Length of Stay by Diagnosis:\n",
    "df.groupBy(\"DIAGNOSIS\").agg(\n",
    "    avg(\"LENGTH_OF_STAY\").alias(\"AVG_LOS\"),\n",
    "    min(\"LENGTH_OF_STAY\").alias(\"MIN_LOS\"),\n",
    "    max(\"LENGTH_OF_STAY\").alias(\"MAX_LOS\")\n",
    ").orderBy(desc(\"AVG_LOS\")).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
