{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f5fd98-b4ae-493c-85e7-feb67d6ef067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, datediff, lag, when, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb91fd6-9421-4627-865e-892916e94713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark=SparkSession.builder.appName(\"ReadmissionRiskPrediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd80087-6316-4efe-9b56-4a36bf676fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ADMISSIONS.csv file into a DataFrame\n",
    "df=spark.read.format(\"csv\") \\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\",True) \\\n",
    "    .load(\"hdfs://namenode:9000/data/ADMISSIONS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641e6b0-2131-406d-a345-253b2254af98",
   "metadata": {},
   "source": [
    "### Readmission Risk Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159971a8-6d9c-4dd2-8b3d-ea6830450004",
   "metadata": {},
   "source": [
    "#### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba81a7f-0dc8-45e9-a82f-8df55821adab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+-------------------+-----------+\n",
      "|SUBJECT_ID|HADM_ID|LENGTH_OF_STAY|NUM_PREV_ADMISSIONS|READMISSION|\n",
      "+----------+-------+--------------+-------------------+-----------+\n",
      "|     10002|  20002|             5|                  0|          0|\n",
      "+----------+-------+--------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert ADMITTIME and DISCHTIME to timestamps\n",
    "df=df.withColumn(\"ADMITTIME\",col(\"ADMITTIME\").cast(\"timestamp\")) \\\n",
    "     .withColumn(\"DISCHTIME\", col(\"DISCHTIME\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate length of stay in days\n",
    "df=df.withColumn(\"LENGTH_OF_STAY\",datediff(col(\"DISCHTIME\"),col(\"ADMITTIME\")))\n",
    "\n",
    "# Calculate number of previous admissions\n",
    "window_spec=Window.partitionBy(\"SUBJECT_ID\").orderBy(\"ADMITTIME\")\n",
    "df=df.withColumn(\"NUM_PREV_ADMISSIONS\",row_number().over(window_spec)-1)\n",
    "\n",
    "# Create the READMISSION label (1 if readmitted within 30 days, 0 otherwise)\n",
    "df=df.withColumn(\"READMISSION\",when(datediff(lag(\"ADMITTIME\").over(window_spec),col(\"DISCHTIME\"))<=30,1).otherwise(0))\n",
    "\n",
    "# Drop rows with null values (e.g., first admission for each patient)\n",
    "df = df.na.drop()\n",
    "# Show the prepared data\n",
    "df.select(\"SUBJECT_ID\", \"HADM_ID\", \"LENGTH_OF_STAY\", \"NUM_PREV_ADMISSIONS\", \"READMISSION\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b0b8436-81b4-44e1-bf0e-9f619065b4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|READMISSION|count|\n",
      "+-----------+-----+\n",
      "|          0|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Count the number of readmissions and non-readmissions\n",
    "readmission_counts=df.groupBy(\"READMISSION\").count()\n",
    "readmission_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd426636-de3b-4182-80da-864da8445fb0",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba2784b-cca3-48f1-9f8e-257d4266d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|features |READMISSION|\n",
      "+---------+-----------+\n",
      "|[5.0,0.0]|0          |\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Define the feature columns\n",
    "feature_columns=[\"LENGTH_OF_STAY\", \"NUM_PREV_ADMISSIONS\"]\n",
    "# Assemble features into a vector\n",
    "assembler=VectorAssembler(inputCols=feature_columns,outputCol=\"features\")\n",
    "df=assembler.transform(df)\n",
    "\n",
    "# Show the DataFrame with features\n",
    "df.select(\"features\", \"READMISSION\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f32498-afae-4ed0-b3a9-2a26cd9e48e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
